{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thermodynamics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "\n",
    "### Peter Onyisi\n",
    "<img src=\"images/texas_logo.png\" width=\"400\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Thermodynamics\" is the term we use for the observed macroscopic behavior of materials, heat, and energy exchange. If you're trying to design steam engines in the Industrial Revolution, this is the level of description that is important.  The fact that \"heat\" is just energy held in externally hard-to-see motions &mdash; so-called \"internal energy\" &mdash; was accepted somewhat slowly, in parallel with the picture of materials being made of atoms; so in thermodynamics energy is separated into \"heat\" (energy transferred between objects by thermal contact), \"internal energy\", and \"work\" (some macroscopic motion, such as an expanding gas pushing a piston).\n",
    "\n",
    "We can relate \"big picture\" observable properties to the microscopic description coming from statistical mechanics.  Here we'll discuss a few.\n",
    "\n",
    "Let's look at the \"Laws of Thermodynamics.\"  (One thing about these laws: they have many, many alternative statements...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Zeroth Law\n",
    "### \"All thermometers are the same\", \"All heat is of the same kind\"\n",
    "\n",
    "The Zeroth Law says that if systems A and B are in thermal equilibrium, and that B and C are in thermal equilibrium, then A and C are in equilibrium.  Our definition of equilibrium here is that their temperatures are the same.  In statistical mechanics the definition of temperature comes from the relationship between the number of states and the internal energy, so this is automatic, but it's not obvious from a large-scale thermodynamic picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The First Law\n",
    "### \"Energy is conserved\", aka \"You can't win\"\n",
    "This is the statement that if a system has internal energy $U$, I put heat $Q$ into it, and the system does work $W$ on its surroundings, then\n",
    "\n",
    "$$ Q = \\Delta U + W $$\n",
    "\n",
    "The law has the informal statement \"you can't win\" - you can't create energy from nothing. If you put heat $Q$ into an engine that returns to its original state after a cycle, and want to get work $W$ out of it, since $\\Delta U$ is zero once the engine is back as it started, $W$ cannot exceed $Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Second Law\n",
    "### \"Heat moves from hotter to colder systems\", \"Entropy increases\", aka \"You can't break even\"\n",
    "This says that the total entropy of an isolated system does not decrease over time, and will increase unless a process is of a special kind called \"reversible.\" In practice essentially everything interesting is not a reversible process - in particular the transfer of heat energy in an engine is not.  If I want to put heat into an engine and get the engine to make work, then I need to have a temperature difference to get the heat to flow, and that means that entropy increases.  So in the end even if I have made an engine that returns back to its original state (hence has the same entropy), the entropy of the larger system including the engine has to increase.\n",
    "\n",
    "The informal statement is \"you can't break even\" - you cannot just turn heat into work, as that would _reduce_ entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Third Law\n",
    "### \"You can't reach absolute zero\", aka \"You can't get out of the game\"\n",
    "\n",
    "This one is a bit more abstract than the others, but basically it says that no physical process can cool a system to absolute zero, because the amount of energy you can remove from a system decreases as the temperature decreases.\n",
    "\n",
    "The informal statement \"you can't get out of the game\" comes from the efficiency of maximally-efficient engine transferring energy from a temperature $T_H$ to a temperature $T_C$ and making work out of it:\n",
    "\n",
    "$$ \\eta = 1 -\\frac{T_C}{T_H} $$\n",
    "\n",
    "(We won't attempt to justify this result, due to Sadi Carnot.)  If the cold reservoir could have a temperature of absolute zero, then the efficiency would be 1 - we could turn all the heat into work.  But if we can never make $T_C$ equal zero, this option is not open to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heat capacity\n",
    "\n",
    "For many systems the energy $\\Delta E$ required to heat them up by $\\Delta T$ is approximately a constant, and the two are related by the formula\n",
    "\n",
    "$$ \\Delta E = C \\Delta T $$\n",
    "\n",
    "where $C$ is called the _heat capacity_.  For example, it takes 1 calorie = 4.184 J of energy to raise the temperature of one gram of water by one degree Kelvin, so the heat capacity of liquid water is 1 cal/K = 4.184 J/K.  (Note this has the same units as Boltzmann's constant...)\n",
    "\n",
    "Heat capacities are measured experimentally, but statistical mechanics gives us a way to relate it to underlying microscopic physics. In ideal gases, for example, assuming volume and number of molecules are held constant, the Sackur-Tetrode equation (and the equipartition principle) gave us\n",
    "\n",
    "$$ E = \\frac{3}{2} N k_B T = \\frac{3}{2} n R T$$\n",
    "\n",
    "where $E$ was the total kinetic energy of the molecules of the ideal gas and $N$ was the number of molecules.  This implies that \n",
    "\n",
    "$$ C = \\frac{3}{2} N k_B = \\frac{3}{2} n R $$\n",
    "\n",
    "for an ideal gas; expressing this as a _molar_ heat capacity (i.e. the heat capacity per mole, in units J K<sup>-1</sup> mol<sup>-1</sup>), we would get \n",
    "\n",
    "$$C_\\mathrm{molar} = \\frac{3}{2} R = 12.5\\ \\textrm{J K}^{-1}\\textrm{ mol}^{-1}$$\n",
    "\n",
    "This is correct for the noble gases at STP!  These are single atoms, so our state counting assumption is correct.  More complex molecules, such as nitrogen (N<sub>2</sub>) have _higher_ heat capacities &mdash; for diatomic gases this changes with temperature and tends to saturate at $\\frac{7}{2} R$ &mdash; which tells us that something more complex is going on (the simple ideal gas model is undercounting the places energy can go).\n",
    "\n",
    "https://en.wikipedia.org/wiki/Molar_heat_capacity#/media/File:DiatomicSpecHeat2.png\n",
    "\n",
    "For crystalline solids, a similar kind of rule applies, except that the molar heat capacity $\\approx 3R$.  This is called the *Dulong-Petit Law*. The extra factor of two comes from the fact that there is energy associated with the displacement of the atoms from their equilibrium positions in the crystalline lattice - so there are six degrees of freedom per atom (the momentum of the atom and the position of the atom), giving us $6 \\times \\frac{1}{2} k_B T$ average energy per atom.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Dulong%E2%80%93Petit_law#/media/File:GraphHeatCapacityElements_SelectedRange.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
